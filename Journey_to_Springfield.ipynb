{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_on_gpu:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "    print('CUDA is available!  Training on GPU ...')"
      ],
      "metadata": {
        "id": "1z_5ooJi-aJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5GKhr3e-SkY"
      },
      "outputs": [],
      "source": [
        "!gdown 1RxBQiZgRAfio2tWhEE7lzZ6IaJzLheH1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile('journey-springfield.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('./content/')"
      ],
      "metadata": {
        "id": "yvw1dDAI-5is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "TRAIN_DIR = Path('/content/content/train')\n",
        "TEST_DIR = Path('/content/content/testset')"
      ],
      "metadata": {
        "id": "JGmLHct3jF3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_val_files = sorted(list(TRAIN_DIR.rglob('*.jpg')))\n",
        "test_files = sorted(list(TEST_DIR.rglob('*.jpg')))"
      ],
      "metadata": {
        "id": "vK-Pv8Fi_NsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#анализ и визуализация исходных данных\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import random\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "\n",
        "def explore_data(train_dir):\n",
        "    all_files = list(Path(train_dir).rglob('*.jpg'))\n",
        "    data = []\n",
        "    for f in all_files:\n",
        "        data.append({'path': str(f), 'character': f.parent.name})\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    print(f\" Всего изображений: {len(df)}\")\n",
        "    print(f\" Количество классов: {df['character'].nunique()}\")\n",
        "\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    char_counts = df['character'].value_counts()\n",
        "    sns.barplot(x=char_counts.values, y=char_counts.index, palette='viridis')\n",
        "    plt.title('Распределение количества изображений по персонажам', fontsize=16)\n",
        "    plt.xlabel('Количество фото', fontsize=12)\n",
        "    plt.ylabel('Персонаж', fontsize=12)\n",
        "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(16, 10))\n",
        "    plt.suptitle('Примеры изображений из датасета', fontsize=20)\n",
        "\n",
        "    unique_chars = df['character'].unique()\n",
        "    sample_chars = random.sample(list(unique_chars), min(15, len(unique_chars)))\n",
        "\n",
        "    for i, char in enumerate(sample_chars):\n",
        "        plt.subplot(3, 5, i + 1)\n",
        "        char_df = df[df['character'] == char]\n",
        "        img_path = random.choice(char_df['path'].values)\n",
        "        img = Image.open(img_path)\n",
        "\n",
        "        plt.imshow(img)\n",
        "        plt.title(f\"{char}\\n(всего: {len(char_df)})\", fontsize=10)\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    plt.show()\n",
        "\n",
        "explore_data(TRAIN_DIR)"
      ],
      "metadata": {
        "id": "mWrsMkuGhM7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from torchvision import models, transforms\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "BATCH_SIZE = 64\n",
        "IMG_SIZE = 224"
      ],
      "metadata": {
        "id": "WaHHwkoZhWr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files = train_val_files\n",
        "labels = [f.parent.name for f in files]\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "encoded_labels = encoder.fit_transform(labels)"
      ],
      "metadata": {
        "id": "3tt_6UJ6hnW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# аугментация и Dataset\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(0.1, 0.1, 0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(IMG_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "class SimpsonsDataset(Dataset):\n",
        "    def __init__(self, paths, labels, transform=None):\n",
        "        self.paths = paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self): return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.paths[idx]).convert('RGB')\n",
        "        if self.transform: img = self.transform(img)\n",
        "        return img, self.labels[idx]"
      ],
      "metadata": {
        "id": "jFkUuMb0FuSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# создание DataLoader с балансировкой\n",
        "train_f, val_f, train_l, val_l = train_test_split(\n",
        "    files, encoded_labels, test_size=0.2, stratify=encoded_labels, random_state=42\n",
        ")\n",
        "\n",
        "weights = 1. / np.bincount(train_l)\n",
        "samples_weights = weights[train_l]\n",
        "sampler = WeightedRandomSampler(samples_weights, len(samples_weights))\n",
        "\n",
        "train_loader = DataLoader(SimpsonsDataset(train_f, train_l, train_transforms), batch_size=BATCH_SIZE, sampler=sampler)\n",
        "val_loader = DataLoader(SimpsonsDataset(val_f, val_l, val_transforms), batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "ZnhdIWgskEPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# создание модели\n",
        "def get_model(num_classes):\n",
        "    model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
        "\n",
        "    # Заменяем классификатор\n",
        "    in_features = model.classifier[1].in_features\n",
        "    model.classifier = nn.Sequential(\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Linear(in_features, num_classes)\n",
        "    )\n",
        "    return model.to(DEVICE)\n",
        "\n",
        "model = get_model(len(encoder.classes_))"
      ],
      "metadata": {
        "id": "-j5YqDh8kfrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#обучение\n",
        "def train_model(model, train_loader, val_loader, epochs=10, lr=1e-3):\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)\n",
        "\n",
        "    history = {'train_loss': [], 'val_f1': []}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for x, y in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            out = model(x)\n",
        "            loss = criterion(out, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "        model.eval()\n",
        "        all_preds, all_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for x, y in val_loader:\n",
        "                out = model(x.to(DEVICE))\n",
        "                all_preds.extend(out.argmax(1).cpu().numpy())\n",
        "                all_labels.extend(y.numpy())\n",
        "\n",
        "        f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Loss {avg_train_loss:.4f} | Val F1: {f1:.4f} | LR: {current_lr:.6f}\")\n",
        "\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "        history['val_f1'].append(f1)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    return history\n",
        "\n",
        "history = train_model(model, train_loader, val_loader, epochs=10)"
      ],
      "metadata": {
        "id": "8LlOg7myzspI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# построение Classification Report\n",
        "model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for x, y in val_loader:\n",
        "        out = model(x.to(DEVICE))\n",
        "        all_preds.extend(out.argmax(1).cpu().numpy())\n",
        "        all_labels.extend(y.numpy())\n",
        "\n",
        "print(classification_report(all_labels, all_preds, target_names=encoder.classes_))"
      ],
      "metadata": {
        "id": "rOSSa4Bdk1Zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_predictions(model, dataset, encoder, n=10):\n",
        "    model.eval()\n",
        "    fig, axes = plt.subplots(2, 5, figsize=(20, 10))\n",
        "    for i in range(n):\n",
        "        img, label = dataset[np.random.randint(len(dataset))]\n",
        "        with torch.no_grad():\n",
        "            output = model(img.unsqueeze(0).to(DEVICE))\n",
        "            pred = output.argmax(1).item()\n",
        "\n",
        "        ax = axes[i//5, i%5]\n",
        "        ax.imshow(img.permute(1, 2, 0).numpy() * 0.2 + 0.5)\n",
        "        color = 'green' if pred == label else 'red'\n",
        "        ax.set_title(f\"True: {encoder.inverse_transform([label])[0]}\\nPred: {encoder.inverse_transform([pred])[0]}\", color=color)\n",
        "        ax.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "v3lrMrP3k9L2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# диагностика и анализ ошибок\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# матрица ошибок\n",
        "def plot_confusion_matrix(model, val_loader, encoder):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            outputs = model(inputs.to(DEVICE))\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.numpy())\n",
        "\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(18, 14))\n",
        "    sns.heatmap(cm, annot=False, fmt='d', cmap='Blues',\n",
        "                xticklabels=encoder.classes_, yticklabels=encoder.classes_)\n",
        "    plt.title('Матрица ошибок: кого с кем путает модель?', fontsize=16)\n",
        "    plt.xlabel('Предсказано', fontsize=12)\n",
        "    plt.ylabel('Истина', fontsize=12)\n",
        "    plt.show()\n",
        "\n",
        "# GRAD-CAM (Визуализация внимания)\n",
        "class GradCAM:\n",
        "    def __init__(self, model, target_layer):\n",
        "        self.model = model\n",
        "        self.target_layer = target_layer\n",
        "        self.gradients = None\n",
        "        self.activations = None\n",
        "        self.hook_layers()\n",
        "\n",
        "    def hook_layers(self):\n",
        "        def forward_hook(module, input, output): self.activations = output\n",
        "        def backward_hook(module, grad_in, grad_out): self.gradients = grad_out[0]\n",
        "\n",
        "        self.target_layer.register_forward_hook(forward_hook)\n",
        "        self.target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "    def generate(self, input_image, class_idx):\n",
        "        output = self.model(input_image)\n",
        "        self.model.zero_grad()\n",
        "        loss = output[0, class_idx]\n",
        "        loss.backward()\n",
        "\n",
        "        weights = torch.mean(self.gradients, dim=(2, 3), keepdim=True)\n",
        "        cam = torch.sum(weights * self.activations, dim=1).squeeze().detach().cpu()\n",
        "        cam = np.maximum(cam, 0)\n",
        "        cam = cam - cam.min()\n",
        "        cam = cam / cam.max()\n",
        "        return cam.numpy()\n",
        "\n",
        "def visualize_gradcam(model, dataset, encoder, n_samples=3):\n",
        "    # Берем последний конволюционный слой (для EfficientNet это features[8])\n",
        "    if hasattr(model, 'backbone'):\n",
        "        target_layer = list(model.backbone.children())[-3]\n",
        "    else:\n",
        "        target_layer = [m for m in model.modules() if isinstance(m, nn.Conv2d)][-1]\n",
        "\n",
        "    cam_engine = GradCAM(model, target_layer)\n",
        "\n",
        "    fig, axes = plt.subplots(n_samples, 2, figsize=(10, 5 * n_samples))\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        idx = np.random.randint(len(dataset))\n",
        "        img_tensor, label_idx = dataset[idx]\n",
        "\n",
        "        model.eval()\n",
        "        output = model(img_tensor.unsqueeze(0).to(DEVICE))\n",
        "        pred_idx = output.argmax(1).item()\n",
        "\n",
        "        mask = cam_engine.generate(img_tensor.unsqueeze(0).to(DEVICE), pred_idx)\n",
        "\n",
        "        img_show = img_tensor.permute(1, 2, 0).numpy()\n",
        "        img_show = np.clip(img_show * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406]), 0, 1)\n",
        "\n",
        "        axes[i, 0].imshow(img_show)\n",
        "        axes[i, 0].set_title(f\"True: {encoder.classes_[label_idx]}\")\n",
        "        axes[i, 0].axis('off')\n",
        "\n",
        "        axes[i, 1].imshow(img_show)\n",
        "        axes[i, 1].imshow(Image.fromarray(np.uint8(255 * mask)).resize((224, 224), resample=Image.BILINEAR),\n",
        "                          cmap='jet', alpha=0.5)\n",
        "        axes[i, 1].set_title(f\"Pred: {encoder.classes_[pred_idx]} (Attention Map)\")\n",
        "        axes[i, 1].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"Отрисовка матрицы ошибок\")\n",
        "plot_confusion_matrix(model, val_loader, encoder)\n",
        "\n",
        "val_ds = SimpsonsDataset(val_f, val_l, val_transforms)\n",
        "\n",
        "print(\"\\n Визуализация Grad-CAM\")\n",
        "visualize_gradcam(model, val_ds, encoder)"
      ],
      "metadata": {
        "id": "yuQnnGXWlCJ0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}